"""
app.py: Flask Web Application for GNN Campaign Recommender

This Flask application provides a web interface to interact with the pre-trained
GraphSAGE campaign recommendation model (trained using main.py).
It allows users to input their profile details via a web form,
loads the necessary model artifacts and graph data, performs on-demand
GNN and SBERT embedding calculations, predicts relevant campaigns,
ranks them based on GNN score, proximity, and semantic similarity,
and displays the top recommendations with explanations.

Requires:
    - Flask, PyTorch, PyTorch Geometric, sentence-transformers, pandas, etc.
      (see requirements.txt)
    - Pre-trained artifacts generated by main.py located in the
      'preprocessed_data_gnn/' directory:
        - recommender_gnn_artifacts_hackathon_final_v2.pkl
        - campaigns_preprocessed_gnn_hackathon_final_v2.csv
        - graph_data_hackathon_final_v2.pt
    - An HTML template at templates/index.html
"""
# -*- coding: utf-8 -*-
# === Imports ===

import pandas as pd
import numpy as np
import os
import joblib
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics.pairwise import cosine_similarity
from datetime import datetime, timedelta
import logging
import torch
import torch.nn.functional as F
from torch.nn import Module, Linear, ReLU, Dropout
from flask import Flask, render_template, request
import collections # Keep this for defaultdict if used in data gen lists
import gc
import random
import time # Import time for potential sleep in debug
from sentence_transformers import SentenceTransformer
try:
    from torch_geometric.nn import SAGEConv
    from torch_geometric.data import Data
except ImportError as e:
    print(f"CRITICAL ERROR: torch-geometric library or dependencies not found: {e}")
    print("Please ensure torch-geometric, torch-scatter, torch-sparse are installed correctly.")
    exit(1)
import warnings
warnings.filterwarnings("ignore", category=FutureWarning)

# === Flask App Setup ===
app = Flask(__name__)

# === Configuration ===
logging.basicConfig(level=logging.INFO, format='%(asctime)s-%(levelname)s-[%(filename)s:%(lineno)d]-%(message)s', datefmt='%Y-%m-%d %H:%M:%S')
logger = logging.getLogger(__name__)

# --- File/Directory Paths ---
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
PREPROCESSED_DIR = os.path.join(BASE_DIR, "preprocessed_data_gnn")
MODEL_ARTIFACTS_FILE = "recommender_gnn_artifacts_hackathon_final_v2.pkl"
CAMPAIGNS_PREPROCESSED_FILE = "campaigns_preprocessed_gnn_hackathon_final_v2.csv"
GRAPH_DATA_FILE = "graph_data_hackathon_final_v2.pt"

# --- Column Names (Match Data Gen Script) ---
USER_ID_COL = 'user_id'
USER_AGE_COL = 'age_group'
USER_LOCATION_COL = 'location'
USER_LOCATION_TIER_COL = 'location_tier' # This column IS generated
USER_DEVICE_COL = 'device_type'
USER_INTERESTS_COL = 'interests'
USER_ACTIVITY_COL = 'activity_level'
USER_MONETARY_COL = 'monetary_level'
USER_PAST_REWARDS_COL = 'past_rewards_list' # This column IS generated
USER_WATCH_CATEGORIES_COL = 'recent_watch_categories' # This column IS generated
USER_WEATHER_COL = 'simulated_weather' # This column IS generated
USER_CREATION_DATE_COL = 'profile_creation_date' # This column IS generated

CAMPAIGN_ID_COL = 'campaign_id'; CAMPAIGN_BUSINESS_NAME_COL = 'business_name'; CAMPAIGN_CATEGORY_COL = 'category'; CAMPAIGN_LOCATION_COL = 'location'; CAMPAIGN_PROMO_COL = 'promo'; CAMPAIGN_TARGET_GROUP_COL = 'target_group'; CAMPAIGN_BUDGET_COL = 'budget'; CAMPAIGN_TARGET_DEVICE_COL = 'target_device' # Used in training script

# --- Device Types & Location Maps (Match Data Gen Script) ---
DEVICE_TYPES = ["Android Smartphone (Mid-Range)", "Android Smartphone (High-End)", "iOS Smartphone", "Tablet (Android)", "Tablet (iOS)", "Windows Desktop/Laptop", "MacOS Desktop/Laptop"] # Match Data Gen

LOCATION_STANDARDIZATION_MAP = {"delhi (ncr)": "delhi", "new delhi": "delhi", "delhi": "delhi", "bangalore (bengaluru)": "bangalore", "bengaluru": "bangalore", "mumbai": "mumbai", "varanasi (benaras)": "varanasi", "benaras": "varanasi", "chennai": "chennai", "kolkata": "kolkata", "hyderabad": "hyderabad", "pune": "pune", "ahmedabad": "ahmedabad", "jaipur": "jaipur", "surat": "surat", "lucknow": "lucknow", "kanpur": "kanpur", "nagpur": "nagpur", "indore": "indore", "thane": "thane", "bhopal": "bhopal", "visakhapatnam": "visakhapatnam", "patna": "patna", "vadodara": "vadodara", "ludhiana": "ludhiana", "agra": "agra", "nashik": "nashik", "coimbatore": "coimbatore", "kochi": "kochi", "chandigarh": "chandigarh", "mysore": "mysore", "amritsar": "amritsar", "guwahati": "guwahati", "shimla": "shimla", "goa": "goa", "rishikesh": "rishikesh", "udaipur": "udaipur", "darjeeling": "darjeeling", "madurai": "madurai", "jodhpur": "jodhpur", "puducherry": "puducherry", "aurangabad": "aurangabad", "dehradun": "dehradun", "bhubaneswar": "bhubaneswar", "raipur": "raipur", "gurgaon": "gurugram", "gurugram": "gurugram", "unknown": "unknown", "nan": "unknown", "": "unknown"}
LOCATION_TIERS_MAP = {"mumbai": "Tier 1", "delhi": "Tier 1", "bangalore": "Tier 1", "chennai": "Tier 1", "kolkata": "Tier 1", "hyderabad": "Tier 1", "pune": "Tier 1", "ahmedabad": "Tier 1", "gurugram": "Tier 1", "noida": "Tier 1", "jaipur": "Tier 2", "surat": "Tier 2", "lucknow": "Tier 2", "kanpur": "Tier 2", "nagpur": "Tier 2", "indore": "Tier 2", "thane": "Tier 2", "bhopal": "Tier 2", "visakhapatnam": "Tier 2", "patna": "Tier 2", "vadodara": "Tier 2", "ludhiana": "Tier 2", "agra": "Tier 2", "nashik": "Tier 2", "coimbatore": "Tier 2", "kochi": "Tier 2", "chandigarh": "Tier 2", "mysore": "Tier 2", "varanasi": "Tier 3", "amritsar": "Tier 3", "guwahati": "Tier 3", "madurai": "Tier 3", "jodhpur": "Tier 3", "aurangabad": "Tier 3", "dehradun": "Tier 3", "bhubaneswar": "Tier 3", "raipur": "Tier 3", "shimla": "Specialty", "goa": "Specialty", "rishikesh": "Specialty", "udaipur": "Specialty", "darjeeling": "Specialty", "puducherry": "Specialty", "unknown": "Unknown"}
LOCATION_REGIONS_MAP = {"NCR": ["delhi", "gurugram", "noida", "faridabad", "ghaziabad"], "Mumbai MMR": ["mumbai", "thane", "navi mumbai"], "South Tier 1": ["bangalore", "chennai", "hyderabad"], "West Tier 1/2": ["pune", "ahmedabad", "surat", "vadodara", "indore", "nagpur", "nashik", "bhopal", "aurangabad"], "North Tier 2/3/S": ["jaipur", "lucknow", "kanpur", "ludhiana", "chandigarh", "agra", "amritsar", "shimla", "dehradun", "rishikesh", "jodhpur"], "East Tier 1/2/3/S": ["kolkata", "patna", "bhubaneswar", "guwahati", "darjeeling", "raipur"], "South Tier 2/3/S": ["visakhapatnam", "coimbatore", "kochi", "mysore", "madurai", "puducherry", "goa", "varanasi"], "Unknown Region": ["unknown"]}

# --- Options for Checkboxes (Derived from Data Gen Script) ---
INTERESTS_OPTIONS = sorted([ # Match Data Gen Script
    "Cricket (IPL fanatic)", "Bollywood Masala", "Regional Cinema (Tamil/Telugu/etc.)", "Indian Politics", "Stock Market (Dalal Street)",
    "Travel (Domestic/International)", "Street Food Safari", "Indian Festivals & Culture",
    "Reality TV (Big Boss)", "Stand-up Comedy (Indian comics)",
    "Yoga & Meditation", "Classical Music/Dance", "Reading (Indian Authors)", "Mythology & History",
    "Ayurveda & Wellness", "Astrology & Vastu", "Traditional Crafts", "Gardening (Home/Organic)",
    "Technology & Gadgets", "Online Gaming (BGMI/FreeFire)", "Social Media (Influencers/Memes)", "Startup Ecosystem",
    "Fitness & Gym", "Sustainable Living", "Online Shopping (Fashion/Electronics)",
    "Learning New Skills/Languages", "DIY & Home Improvement", "Photography",
    "Cooking & Recipes (Regional/Global)", "Blogging/Vlogging", "Cars & Bikes", "Parenting"
    ,"Health & Nutrition"
])
# Reconstruct WATCH_CATEGORY_OPTIONS from data gen logic
DEFAULT_WATCH_CATEGORIES_GEN = ["comedy sketches (indian creators)", "trending music videos (india)", "whatsapp status videos", "news bulletins", "devotional songs (bhajans)", "short films"]
INTEREST_TO_WATCH_MAP_GEN = collections.defaultdict(lambda: DEFAULT_WATCH_CATEGORIES_GEN)
INTEREST_TO_WATCH_MAP_GEN['cricket (ipl fanatic)'] = ["ipl match highlights", "expert analysis shows", "fantasy league tips", "cricketer interviews", "live match streams"]
INTEREST_TO_WATCH_MAP_GEN['bollywood masala'] = ["latest movie trailers", "song releases", "celebrity gossip", "film award shows", "movie reviews", "behind the scenes"]
INTEREST_TO_WATCH_MAP_GEN['regional cinema (tamil/telugu/etc.)'] = ["regional movie trailers", "tollywood/kollywood news", "actor interviews", "audio launches", "regional movie streams"]
INTEREST_TO_WATCH_MAP_GEN['indian politics'] = ["political debates", "election analysis", "news headlines", "parliament sessions", "interviews with politicians"]
INTEREST_TO_WATCH_MAP_GEN['travel (domestic/international)'] = ["travel vlogs (india/abroad)", "destination guides", "travel tips & hacks", "road trip videos", "hotel/resort reviews"]
INTEREST_TO_WATCH_MAP_GEN['street food safari'] = ["food review vlogs (india)", "street food discovery", "regional recipe tutorials", "restaurant reviews", "cooking shows"]
INTEREST_TO_WATCH_MAP_GEN['yoga & meditation'] = ["yoga tutorials (hindi/english)", "guided meditation", "pranayama techniques", "spiritual discourses", "wellness retreats info"]
INTEREST_TO_WATCH_MAP_GEN['online gaming (bgmi/freefire)'] = ["gaming streams (indian gamers)", "gameplay highlights", "esports tournaments (india)", "new game reviews", "gaming setup tours"]
INTEREST_TO_WATCH_MAP_GEN['fitness & gym'] = ["home workout routines (india focus)", "gym motivation videos", "indian fitness influencers", "healthy indian recipes", "supplement reviews"]
INTEREST_TO_WATCH_MAP_GEN['cooking & recipes (regional/global)'] = ["north indian recipes", "south indian cooking", "bengali sweets tutorials", "maharashtrian thali", "quick snack recipes", "baking tutorials"]
INTEREST_TO_WATCH_MAP_GEN['online shopping (fashion/electronics)'] = ["product unboxing videos", "fashion hauls (myntra/ajio)", "gadget reviews", "sale alert videos", "comparison videos"]
INTEREST_TO_WATCH_MAP_GEN['stock market (dalal street)'] = ["stock analysis videos", "investment strategies", "ipo reviews", "economic news commentary", "trading tutorials"]
ALL_WATCH_CATEGORIES_GEN = list(set([cat for sublist in INTEREST_TO_WATCH_MAP_GEN.values() for cat in sublist] + DEFAULT_WATCH_CATEGORIES_GEN))
WATCH_CATEGORY_OPTIONS = sorted(ALL_WATCH_CATEGORIES_GEN)

# --- Hardware/Embedding Setup ---
DEVICE = torch.device('cpu')
logger.info(f"Flask app using device: {DEVICE}")
SBERT_MODEL_NAME = 'all-MiniLM-L6-v2'
sbert_model = None
EMBEDDING_DIM = 384

# --- Load Artifacts and Model (Global Variables) ---
loaded_artifacts = None
campaigns_df_preprocessed = None
model = None
predictor = None
graph_node_features = None
graph_edge_index = None

# GNN Model Definitions
class GraphSAGE_GNN(Module):
    def __init__(self, in_channels, hidden_channels, out_channels, dropout=0.5): super().__init__(); self.conv1 = SAGEConv(in_channels, hidden_channels); self.conv2 = SAGEConv(hidden_channels, out_channels); self.dropout = Dropout(dropout);
    def forward(self, x, edge_index): x = F.relu(self.conv1(x, edge_index)); x = self.dropout(x); x = self.conv2(x, edge_index); return x
class LinkPredictor(Module):
    def __init__(self, in_channels): super().__init__(); predictor_hidden_dim = max(32, in_channels // 2); self.lin1 = Linear(in_channels * 2, predictor_hidden_dim); self.lin2 = Linear(predictor_hidden_dim, 1); self.dropout = Dropout(0.3);
    def forward(self, x_i, x_j): x = torch.cat([x_i, x_j], dim=-1); x = F.relu(self.lin1(x)); x = self.dropout(x); x = self.lin2(x); return x.squeeze(-1)

# === Helper Functions ===
def clear_flask_context_cache(): gc.collect(); logger.debug("Performed gc.collect()")
def get_sbert_batch_embeddings(texts, model, batch_size=128):
    global EMBEDDING_DIM
    if not model: logger.error("SBERT model not loaded!"); return [np.zeros(EMBEDDING_DIM)] * len(texts)
    if not isinstance(texts, list): texts = list(texts)
    processed_texts = [str(t).strip() if pd.notna(t) and str(t).strip() else " " for t in texts]
    original_indices = [i for i, t in enumerate(processed_texts) if t != " "]
    embeddings = np.zeros((len(texts), EMBEDDING_DIM), dtype=np.float32)
    valid_texts = [processed_texts[i] for i in original_indices]
    if not valid_texts: return list(embeddings)
    try:
        show_bar = False
        with torch.no_grad(): valid_embeddings = model.encode(valid_texts, convert_to_numpy=True, device=DEVICE, batch_size=batch_size, show_progress_bar=show_bar)
        if valid_embeddings.shape[0] != len(valid_texts): logger.warning(f"SBERT encode mismatch!")
        for i, embed_idx in enumerate(original_indices):
             if i < len(valid_embeddings): embeddings[embed_idx] = valid_embeddings[i]
        return list(embeddings)
    except RuntimeError as e: logger.warning(f"Runtime error SBERT: {e}.");
    except Exception as e: logger.warning(f"General error SBERT: {e}.");
    return list(np.zeros((len(texts), EMBEDDING_DIM), dtype=np.float32))
def get_sbert_embedding_pred(text, model): return get_sbert_batch_embeddings([text], model, batch_size=1)[0]
def get_sbert_list_embedding_pred(list_text, model):
    global EMBEDDING_DIM
    if not model: return np.zeros(EMBEDDING_DIM)
    if isinstance(list_text, list): items = [item.strip() for item in list_text if isinstance(item, str) and item.strip()]
    elif isinstance(list_text, str) and list_text.strip(): items = [item.strip() for item in list_text.split(',') if item.strip()]
    else: items = []
    if not items: return np.zeros(EMBEDDING_DIM)
    item_embeddings_list = get_sbert_batch_embeddings(items, model, batch_size=len(items))
    item_embeddings = np.array(item_embeddings_list); valid_mask = np.any(item_embeddings != 0, axis=1)
    if not np.any(valid_mask): return np.zeros(EMBEDDING_DIM)
    avg_embedding = np.mean(item_embeddings[valid_mask], axis=0)
    if np.any(~np.isfinite(avg_embedding)): return np.zeros(EMBEDDING_DIM)
    return avg_embedding
def standardize_location_pred(loc_str): return LOCATION_STANDARDIZATION_MAP.get(str(loc_str).lower().strip(), str(loc_str).lower().strip())
def get_location_region_pred(standardized_location):
    for region, locations in LOCATION_REGIONS_MAP.items():
        if standardized_location in locations: return region
    return "Other"

# === Load Model and Data Function ===
def load_model_and_data():
    global loaded_artifacts, campaigns_df_preprocessed, model, predictor, sbert_model, EMBEDDING_DIM, graph_node_features, graph_edge_index
    func_name="load_model_and_data"; logger.info(f"[{func_name}] Starting App Init...")
    try:
        logger.info(f"[{func_name}] Loading SBERT model...")
        sbert_model = SentenceTransformer(SBERT_MODEL_NAME, device=DEVICE)
        EMBEDDING_DIM = sbert_model.get_sentence_embedding_dimension()
        logger.info(f"[{func_name}] Loading artifacts...")
        artifact_path = os.path.join(PREPROCESSED_DIR, MODEL_ARTIFACTS_FILE)
        if not os.path.exists(artifact_path): raise FileNotFoundError(f"Artifact file missing: {artifact_path}")
        loaded_artifacts = joblib.load(artifact_path)
        logger.info(f"[{func_name}] Loading preprocessed campaigns...")
        campaign_path = os.path.join(PREPROCESSED_DIR, CAMPAIGNS_PREPROCESSED_FILE)
        if not os.path.exists(campaign_path): raise FileNotFoundError(f"Campaigns preprocessed file missing: {campaign_path}")
        campaigns_df_preprocessed = pd.read_csv(campaign_path)
        req_camp_cols = [CAMPAIGN_ID_COL, CAMPAIGN_PROMO_COL, CAMPAIGN_CATEGORY_COL, CAMPAIGN_LOCATION_COL]
        if not all(col in campaigns_df_preprocessed.columns for col in req_camp_cols): raise ValueError(f"Preprocessed campaigns CSV missing columns.")
        campaigns_df_preprocessed[CAMPAIGN_ID_COL] = campaigns_df_preprocessed[CAMPAIGN_ID_COL].astype(str)
        campaigns_df_preprocessed = campaigns_df_preprocessed.set_index(CAMPAIGN_ID_COL, drop=False)
        logger.info(f"[{func_name}] Loading graph structure...")
        graph_path = os.path.join(PREPROCESSED_DIR, GRAPH_DATA_FILE)
        if not os.path.exists(graph_path): raise FileNotFoundError(f"Graph data file missing: {graph_path}")
        graph_data = torch.load(graph_path, map_location=DEVICE)
        required_attrs = ['x', 'train_pos_edge_index']
        if not all(hasattr(graph_data, attr) and getattr(graph_data, attr) is not None for attr in required_attrs): raise ValueError(f"Graph data missing features (x) or edges.")
        graph_node_features = graph_data.x.to(DEVICE)
        graph_edge_index = graph_data.train_pos_edge_index.to(DEVICE)
        logger.info(f"[{func_name}] Graph structure loaded: Nodes={graph_node_features.shape[0]}, Edges={graph_edge_index.shape[1]}")
        del graph_data; gc.collect()
        logger.info(f"[{func_name}] Rebuilding model structure...")
        training_hp = loaded_artifacts.get('training_hyperparams', {}); node_feature_dim = loaded_artifacts.get('node_feature_dim')
        if node_feature_dim is None: raise ValueError("node_feature_dim missing from artifacts.")
        if graph_node_features is not None and node_feature_dim != graph_node_features.shape[1]: raise ValueError(f"Node feature dimension mismatch! Artifacts={node_feature_dim}, GraphData={graph_node_features.shape[1]}")
        hidden_channels = training_hp.get('hidden', 64); out_channels = training_hp.get('out', 32); dropout = training_hp.get('dropout', 0.5); model_type = training_hp.get('model_type', 'SAGE');
        if model_type.upper() != 'SAGE': raise ValueError(f"Unsupported model type '{model_type}'")
        model = GraphSAGE_GNN(node_feature_dim, hidden_channels, out_channels, dropout).to(DEVICE)
        predictor = LinkPredictor(out_channels).to(DEVICE)
        if 'model_state_dict' not in loaded_artifacts or 'predictor_state_dict' not in loaded_artifacts: raise ValueError("Model states missing from artifacts.")
        model.load_state_dict(loaded_artifacts['model_state_dict']); predictor.load_state_dict(loaded_artifacts['predictor_state_dict']); model.eval(); predictor.eval()
        logger.info(f"[{func_name}] Model loaded.")
        clear_flask_context_cache()
        logger.info(f"[{func_name}] Initialization successful! (Embeddings will be computed on demand)")
    except FileNotFoundError as fnf_e: logger.error(f"[{func_name}] ERROR: Required file not found."); logger.error(fnf_e); raise SystemExit(f"Critical file missing: {fnf_e}") from fnf_e
    except Exception as e: logger.error(f"[{func_name}] Error during loading: {e}", exc_info=True); raise SystemExit(f"Failed to initialize: {e}") from e

# === Prediction Function ===
@torch.no_grad()
def predict_campaigns_gnn_flask(user_data: dict):
    global loaded_artifacts, campaigns_df_preprocessed, model, predictor, sbert_model, graph_node_features, graph_edge_index
    func_name = "predict_campaigns_gnn_flask"; logger.info(f"[{func_name}] User: {user_data.get(USER_ID_COL, 'N/A')}")
    if not all([loaded_artifacts, campaigns_df_preprocessed is not None, model, predictor, sbert_model, graph_node_features is not None, graph_edge_index is not None]):
        logger.error(f"[{func_name}] Critical resources not loaded."); return [], 0.0, "Error: Server resources not ready."

    start_total_time = datetime.now()
    try:
        # --- 1. User Data Preprocessing & Tier Calc (REVISED) ---
        user_df = pd.DataFrame([user_data])
        user_defaults_pred_scalar = { USER_AGE_COL: 'unknown', USER_LOCATION_COL: 'unknown', USER_ACTIVITY_COL: 'Medium (Weekly)', USER_MONETARY_COL: 'Medium Spender', USER_DEVICE_COL: 'Android Smartphone (Mid-Range)', USER_WEATHER_COL: 'Clear Skies'}
        for col, default in user_defaults_pred_scalar.items():
            if col not in user_df.columns: user_df[col] = default
        user_df.fillna(user_defaults_pred_scalar, inplace=True)
        for col in [USER_INTERESTS_COL, USER_WATCH_CATEGORIES_COL]:
            if col not in user_df.columns: user_df[col] = pd.Series([[] for _ in range(len(user_df))], index=user_df.index)
            else: user_df[col] = user_df[col].apply(lambda x: x if isinstance(x, list) else [])
        for col in [USER_LOCATION_COL, USER_ACTIVITY_COL, USER_MONETARY_COL]: user_df[col] = user_df[col].astype(str).str.lower().str.strip()
        user_loc_raw = user_df[USER_LOCATION_COL].iloc[0]; user_loc_standardized = standardize_location_pred(user_loc_raw); user_region = get_location_region_pred(user_loc_standardized)
        user_activity_level = user_df[USER_ACTIVITY_COL].iloc[0]
        logger.info(f"[{func_name}] User Loc:'{user_loc_raw}'->Std:'{user_loc_standardized}'->Region:'{user_region}'. Activity:'{user_activity_level}'")
        user_interest_data = user_df[USER_INTERESTS_COL].iloc[0]
        user_watch_data = user_df[USER_WATCH_CATEGORIES_COL].iloc[0]

        # --- 2. Calculate GNN Embeddings ON DEMAND ---
        logger.info(f"[{func_name}] Calculating GNN embeddings...")
        start_time_gnn = datetime.now()
        x_feat = graph_node_features.to(DEVICE)
        edge_idx = graph_edge_index.to(DEVICE)
        if not x_feat.is_contiguous(): x_feat = x_feat.contiguous()
        if not edge_idx.is_contiguous(): edge_idx = edge_idx.contiguous()
        all_node_embeddings = model(x_feat, edge_idx)
        if id(x_feat) != id(graph_node_features): del x_feat
        if id(edge_idx) != id(graph_edge_index): del edge_idx
        gc.collect(); end_time_gnn = datetime.now()
        logger.info(f"[{func_name}] GNN embedding calculation took: {(end_time_gnn - start_time_gnn).total_seconds():.2f}s")

        # --- 3. Calculate GNN Scores & Avg Relevance ---
        logger.debug(f"[{func_name}] Calculating GNN scores & avg relevance...");
        num_users = loaded_artifacts['num_users']; num_campaigns = loaded_artifacts['num_campaigns']; campaign_indices = list(range(num_users, num_users + num_campaigns))
        if not campaign_indices or max(campaign_indices) >= all_node_embeddings.shape[0]: raise ValueError("Invalid campaign indices")
        user_orig_id = user_df.iloc[0].get(USER_ID_COL); user_map = loaded_artifacts['user_map']; user_node_idx = user_map.get(str(user_orig_id)) if user_orig_id else None
        if user_node_idx is not None and user_node_idx < num_users: user_embedding = all_node_embeddings[user_node_idx].unsqueeze(0); logger.info(f"[{func_name}] Found known user '{user_orig_id}'.")
        else: logger.warning(f"[{func_name}] User '{user_orig_id}' unknown. Using average embedding."); user_embedding = all_node_embeddings[:num_users].mean(dim=0, keepdim=True) if num_users > 0 else torch.zeros((1, all_node_embeddings.shape[1]), device=DEVICE);
        campaign_embeddings_gnn = all_node_embeddings[campaign_indices]; user_embedding_repeated = user_embedding.repeat(len(campaign_indices), 1);
        all_scores = predictor(user_embedding_repeated, campaign_embeddings_gnn); all_scores_sigmoid = torch.sigmoid(all_scores); all_scores_np = all_scores_sigmoid.cpu().numpy()
        avg_gnn_score_threshold = 0.5; relevant_scores = all_scores_np[all_scores_np > avg_gnn_score_threshold]
        avg_relevance_score = np.mean(relevant_scores) if relevant_scores.size > 0 else (np.mean(all_scores_np) if all_scores_np.size > 0 else 0.0)
        logger.info(f"[{func_name}] User Avg Relevance Score: {avg_relevance_score:.4f}")

        # --- 4. Calculate Dynamic User Tier ---
        high_activity_keywords = ['very high', 'daily', 'high', 'multiple weekly']; medium_activity_keywords = ['medium', 'weekly']
        score_threshold_elite = 0.75; score_threshold_prem = 0.60; dynamic_user_tier = "Basic"
        is_high_activity = any(keyword in user_activity_level for keyword in high_activity_keywords); is_medium_activity = any(keyword in user_activity_level for keyword in medium_activity_keywords)
        if is_high_activity and avg_relevance_score >= score_threshold_prem: dynamic_user_tier = "Elite"
        elif (is_medium_activity and avg_relevance_score >= score_threshold_elite) or (avg_relevance_score >= score_threshold_elite): dynamic_user_tier = "Premium"
        logger.info(f"[{func_name}] Calculated dynamic user tier: {dynamic_user_tier}")

        # --- 5. Get User SBERT Embeddings ON DEMAND ---
        logger.debug(f"[{func_name}] Generating SBERT embeddings for user input..."); start_time_sbert_user = datetime.now()
        user_interest_emb_np = get_sbert_list_embedding_pred(user_interest_data, sbert_model).reshape(1, -1)
        user_watch_emb_np = get_sbert_list_embedding_pred(user_watch_data, sbert_model).reshape(1, -1)
        end_time_sbert_user = datetime.now(); logger.info(f"[{func_name}] User SBERT calculation took: {(end_time_sbert_user - start_time_sbert_user).total_seconds():.2f}s")


        # --- 6. Prepare Candidate List (Calculate Campaign SBERT & Similarities ON DEMAND) ---
        logger.info(f"[{func_name}] Calculating proximity and semantic scores for {num_campaigns} campaigns...")
        campaign_locations = campaigns_df_preprocessed[CAMPAIGN_LOCATION_COL].astype(str).to_dict(); reverse_campaign_map = loaded_artifacts['reverse_campaign_map']; all_candidates_info = []
        campaign_ids_in_order = []; promo_texts_to_embed = []; category_texts_to_embed = []; gnn_scores_in_order = []
        for i, gnn_score_val in enumerate(all_scores_np):
             campaign_node_idx = campaign_indices[i]; original_campaign_id = reverse_campaign_map.get(campaign_node_idx)
             if not original_campaign_id: continue
             campaign_ids_in_order.append(original_campaign_id); gnn_scores_in_order.append(float(gnn_score_val))
             try:
                 promo_texts_to_embed.append(campaigns_df_preprocessed.loc[original_campaign_id, CAMPAIGN_PROMO_COL])
                 category_texts_to_embed.append(campaigns_df_preprocessed.loc[original_campaign_id, CAMPAIGN_CATEGORY_COL])
             except KeyError: logger.warning(f"Missing campaign {original_campaign_id}, skipping SBERT."); promo_texts_to_embed.append(""); category_texts_to_embed.append("")
        logger.info(f"[{func_name}] Calculating campaign SBERT embeddings..."); start_time_sbert_camp = datetime.now()
        campaign_promo_embeds_list = get_sbert_batch_embeddings(promo_texts_to_embed, sbert_model)
        campaign_category_embeds_list = get_sbert_batch_embeddings(category_texts_to_embed, sbert_model)
        end_time_sbert_camp = datetime.now(); logger.info(f"[{func_name}] Campaign SBERT calculation took: {(end_time_sbert_camp - start_time_sbert_camp).total_seconds():.2f}s")
        campaign_promo_embeddings = {cid: emb for cid, emb in zip(campaign_ids_in_order, campaign_promo_embeds_list)}
        campaign_category_embeddings = {cid: emb for cid, emb in zip(campaign_ids_in_order, campaign_category_embeds_list)}

        for i, original_campaign_id in enumerate(campaign_ids_in_order):
            gnn_score = gnn_scores_in_order[i]
            campaign_loc_std = campaign_locations.get(original_campaign_id, 'unknown'); campaign_region = get_location_region_pred(campaign_loc_std)
            proximity_score = 2 if campaign_loc_std == user_loc_standardized else (1 if campaign_region != "Other" and campaign_region != "Unknown Region" and campaign_region == user_region else 0)
            interest_promo_sim = 0.0; watch_cat_sim = 0.0
            camp_promo_embed = campaign_promo_embeddings.get(original_campaign_id); camp_cat_embed = campaign_category_embeddings.get(original_campaign_id)
            try:
                if user_interest_emb_np.size > 1 and camp_promo_embed is not None and camp_promo_embed.size > 1 and user_interest_emb_np.shape[1] == camp_promo_embed.shape[0]: interest_promo_sim = cosine_similarity(user_interest_emb_np, camp_promo_embed.reshape(1, -1))[0][0]
                if user_watch_emb_np.size > 1 and camp_cat_embed is not None and camp_cat_embed.size > 1 and user_watch_emb_np.shape[1] == camp_cat_embed.shape[0]: watch_cat_sim = cosine_similarity(user_watch_emb_np, camp_cat_embed.reshape(1, -1))[0][0]
            except Exception as sim_e: logger.warning(f"Sim calc error for {original_campaign_id}: {sim_e}")
            all_candidates_info.append({'campaign_id': original_campaign_id, 'gnn_score': gnn_score, 'interest_promo_sim': max(0, float(interest_promo_sim)), 'watch_cat_sim': max(0, float(watch_cat_sim)), 'location': campaign_loc_std, 'proximity': proximity_score })

        # --- 7. Separate and Rank within Groups ---
        logger.info(f"[{func_name}] Ranking candidates within proximity groups..."); exact_matches = []; region_matches = []
        for cand in all_candidates_info:
            if cand['proximity'] == 2: exact_matches.append(cand)
            elif cand['proximity'] == 1: region_matches.append(cand)
        exact_matches.sort(key=lambda x: (x['watch_cat_sim'], x['interest_promo_sim']), reverse=True)
        region_matches.sort(key=lambda x: (x['watch_cat_sim'], x['interest_promo_sim']), reverse=True)
        logger.debug(f"Ranked {len(exact_matches)} exact matches, {len(region_matches)} region matches.")

        # --- 8. Build Final List ---
        logger.info(f"[{func_name}] Building final recommendation list..."); final_recommendations_candidates = []; processed_ids = set(); num_recommendations = 10
        for cand in exact_matches:
            if len(final_recommendations_candidates) >= num_recommendations: break
            if cand['campaign_id'] not in processed_ids: final_recommendations_candidates.append(cand); processed_ids.add(cand['campaign_id'])
        if len(final_recommendations_candidates) < num_recommendations:
             needed = num_recommendations - len(final_recommendations_candidates); added_regional = 0
             for cand in region_matches:
                 if added_regional >= needed: break
                 if cand['campaign_id'] not in processed_ids: final_recommendations_candidates.append(cand); processed_ids.add(cand['campaign_id']); added_regional += 1
        logger.info(f"Final candidate list size: {len(final_recommendations_candidates)}")

        # --- 9. Format Output & Calculate Satisfaction (DETAILED Explainability) ---
        logger.info(f"[{func_name}] Formatting final output with detailed explanations..."); results_list = []; total_weighted_score = 0
        proximity_satisfaction_weights = {2: 1.0, 1: 0.75, 0: 0.1}; SIM_THRESHOLD = 0.25; HIGH_GNN_THRESHOLD = 0.7
        for rank, candidate in enumerate(final_recommendations_candidates):
            original_campaign_id = candidate['campaign_id']
            try:
                campaign_info_series = campaigns_df_preprocessed.loc[original_campaign_id]; display_score = candidate['gnn_score']
                reason_parts = []; proximity_level = candidate['proximity']; watch_sim = candidate['watch_cat_sim']; interest_sim = candidate['interest_promo_sim']; gnn_score_val = candidate['gnn_score']
                if proximity_level == 2: reason_parts.append("Exact Location Match")
                elif proximity_level == 1: reason_parts.append("Region Match")
                is_watch_strong = watch_sim >= SIM_THRESHOLD; is_interest_strong = interest_sim >= SIM_THRESHOLD
                if is_watch_strong: reason_parts.append(f"Strong Watch History Match ({watch_sim:.2f})")
                elif is_interest_strong: reason_parts.append(f"Strong Interest Match ({interest_sim:.2f})")
                if not is_watch_strong and not is_interest_strong:
                    if proximity_level > 0 and gnn_score_val >= HIGH_GNN_THRESHOLD: reason_parts.append(f"High Overall Score ({gnn_score_val:.2f})")
                    elif proximity_level == 0: reason_parts.append(f"Overall Relevance ({gnn_score_val:.2f})")
                reason = " + ".join(reason_parts) if reason_parts else "General Recommendation"
                results_list.append({"campaign_id": original_campaign_id, "business_name": campaign_info_series.get(CAMPAIGN_BUSINESS_NAME_COL, "N/A"), "promo": campaign_info_series.get(CAMPAIGN_PROMO_COL, "N/A"), "category": campaign_info_series.get(CAMPAIGN_CATEGORY_COL, "N/A"), "location": candidate['location'], "proximity_level": proximity_level, "score": round(display_score, 4), "tier": dynamic_user_tier, "interest_promo_sim": round(interest_sim, 4), "watch_cat_sim": round(watch_sim, 4), "reason": reason })
                total_weighted_score += display_score * proximity_satisfaction_weights.get(proximity_level, 0.1)
            except KeyError: logger.warning(f"Campaign ID {original_campaign_id} not found in preprocessed index."); continue
            except Exception as e: logger.warning(f"Error formatting final rec {original_campaign_id}: {e}"); continue
        satisfaction_overall = (total_weighted_score / len(results_list)) * 100 if results_list else 0.0
        num_exact = sum(1 for r in results_list if r['proximity_level'] == 2); num_region = sum(1 for r in results_list if r['proximity_level'] == 1)
        logger.info(f"[{func_name}] Prediction successful. Recs: {len(results_list)} (Exact: {num_exact}, Region: {num_region}). Final Weighted Satisfaction: {satisfaction_overall:.2f}%.")

        # Explicitly delete large tensors
        if 'all_node_embeddings' in locals(): del all_node_embeddings
        if 'user_embedding' in locals(): del user_embedding
        if 'campaign_embeddings_gnn' in locals(): del campaign_embeddings_gnn
        if 'all_scores' in locals(): del all_scores
        if 'all_scores_sigmoid' in locals(): del all_scores_sigmoid
        if 'campaign_promo_embeddings' in locals(): del campaign_promo_embeddings
        if 'campaign_category_embeddings' in locals(): del campaign_category_embeddings
        gc.collect()
        end_total_time = datetime.now()
        logger.info(f"[{func_name}] Total prediction request time: {(end_total_time - start_total_time).total_seconds():.2f}s")
        return results_list, satisfaction_overall, None
    except Exception as e:
        logger.error(f"[{func_name}] Prediction Error: {e}", exc_info=True)
        if 'all_node_embeddings' in locals(): del all_node_embeddings
        if 'campaign_promo_embeddings' in locals(): del campaign_promo_embeddings
        if 'campaign_category_embeddings' in locals(): del campaign_category_embeddings
        gc.collect()
        return [], 0.0, f'Prediction failed: {type(e).__name__} - {e}'
    finally:
        clear_flask_context_cache()

# === Flask Routes ===
def _get_dropdown_options():
    options = {'common_interests': INTERESTS_OPTIONS, 'common_watch_categories': WATCH_CATEGORY_OPTIONS}
    error_msg = None
    try:
        if loaded_artifacts is None: raise RuntimeError("Model artifacts not loaded.")
        location_encoder = loaded_artifacts.get('location_encoder')
        options['locations'] = sorted([loc.title() for loc in location_encoder.classes_ if loc != 'unknown']) if location_encoder else ['Enter Manually']
        age_encoder = loaded_artifacts.get(f'{USER_AGE_COL}_encoder')
        options['age_groups'] = [opt.title() for opt in age_encoder.classes_] if age_encoder else ["16-22 (Student)", "23-30 (Young Pro)", "31-40 (Settling Down)", "41-55 (Established)", "56-65 (Senior)", "65+ (Retired)"]
        device_encoder = loaded_artifacts.get(f'{USER_DEVICE_COL}_encoder')
        options['device_types'] = [opt.title() for opt in device_encoder.classes_] if device_encoder else ["Android Smartphone (Mid-Range)", "Android Smartphone (High-End)", "iOS Smartphone", "Tablet (Android)", "Tablet (iOS)", "Windows Desktop/Laptop", "MacOS Desktop/Laptop"]
        activity_encoder = loaded_artifacts.get(f'{USER_ACTIVITY_COL}_encoder')
        options['activity_levels'] = [opt.title() for opt in activity_encoder.classes_] if activity_encoder else ["Very High (Daily+)", "High (Multiple Weekly)", "Medium (Weekly)", "Low (Monthly)", "Very Low (Infrequent)"]
        monetary_encoder = loaded_artifacts.get(f'{USER_MONETARY_COL}_encoder')
        options['monetary_levels'] = [opt.title() for opt in monetary_encoder.classes_] if monetary_encoder else ["Low Spender", "Medium Spender", "High Spender", "Very High Spender"]
        weather_encoder = loaded_artifacts.get(f'{USER_WEATHER_COL}_encoder')
        options['weather_options'] = [opt.title() for opt in weather_encoder.classes_] if weather_encoder else ["Scorching Heat", "Humid & Sticky", "Pleasant Breeze", "Monsoon Downpour", "Winter Fog (North India)", "Dry Heat (Rajasthan)", "Coastal Humidity", "Hill Station Chill", "Clear Skies", "Overcast"]
        for k, v in options.items():
             if not v and k not in ['locations', 'common_interests', 'common_watch_categories']: logger.warning(f"No options loaded for dropdown '{k}' from artifacts, using fallback.")
    except Exception as e:
        logger.error(f"Error populating dropdown options: {e}")
        error_msg = "Error loading dropdown options. Using defaults."
        options = { 'locations': ['Enter Manually'], 'age_groups': ["16-22 (Student)", "23-30 (Young Pro)"], 'device_types': ["Android Smartphone (Mid-Range)", "iOS Smartphone"], 'activity_levels': ["Medium (Weekly)"], 'monetary_levels': ["Medium Spender"], 'weather_options': ["Clear Skies"], 'common_interests': INTERESTS_OPTIONS, 'common_watch_categories': WATCH_CATEGORY_OPTIONS }
    finally:
        default_options_structure = { 'locations': ['Enter Manually'], 'age_groups': [], 'device_types': [], 'activity_levels': [], 'monetary_levels': [], 'weather_options': [], 'common_interests': INTERESTS_OPTIONS, 'common_watch_categories': WATCH_CATEGORY_OPTIONS }
        for key in default_options_structure:
            if key not in options: options[key] = default_options_structure[key]
    return options, error_msg

@app.route('/')
def index():
    options, error_msg = _get_dropdown_options()
    return render_template('index.html', results=None, error=error_msg, submitted=False, options=options)

@app.route('/predict', methods=['POST'])
def predict():
    error_msg = None; recommendations = []; satisfaction=0.0
    options, error_msg_opts = _get_dropdown_options()
    if error_msg_opts: return render_template('index.html', results=[], error=error_msg_opts, submitted=True, satisfaction=0, options=options)
    try:
        user_data = {}; form_ok = True
        required_fields = { USER_LOCATION_COL: request.form.get('location'), USER_ACTIVITY_COL: request.form.get('activity_level') }
        for key, value in required_fields.items():
            if not value: error_msg = f"Please select a value for {key.replace('_',' ').title()}."; form_ok = False; break
            user_data[key] = value
        if form_ok:
            user_data[USER_ID_COL] = request.form.get('user_id', f"ANON_{random.randint(1000,9999)}").strip()
            user_data[USER_AGE_COL] = request.form.get('age_group', 'unknown')
            user_data[USER_DEVICE_COL] = request.form.get('device_type', 'unknown')
            user_data[USER_MONETARY_COL] = request.form.get('monetary_level', 'unknown')
            user_data[USER_WEATHER_COL] = request.form.get('simulated_weather', 'unknown')
            interests_list = request.form.getlist(USER_INTERESTS_COL)
            watch_list = request.form.getlist(USER_WATCH_CATEGORIES_COL)
            user_data[USER_INTERESTS_COL] = interests_list if interests_list else []
            user_data[USER_WATCH_CATEGORIES_COL] = watch_list if watch_list else []
            logger.info(f"Received prediction request for user data: { {k:v for k,v in user_data.items() if k not in [USER_INTERESTS_COL, USER_WATCH_CATEGORIES_COL]} } ...") # Log without long lists
            recommendations, satisfaction, pred_error_msg = predict_campaigns_gnn_flask(user_data)
            if pred_error_msg: error_msg = pred_error_msg
        elif not error_msg: error_msg = "Form validation failed. Please check required fields."
    except Exception as e: logger.error(f"Error in /predict route processing: {e}", exc_info=True); error_msg = f"An unexpected server error occurred."
    submitted_data = request.form.to_dict(flat=False)
    for key, value in submitted_data.items():
        if isinstance(value, list) and len(value) == 1 and key not in [USER_INTERESTS_COL, USER_WATCH_CATEGORIES_COL]: submitted_data[key] = value[0]
    return render_template('index.html', results=recommendations, error=error_msg, submitted=True, satisfaction=satisfaction, submitted_data=submitted_data, options=options)

# === Main Execution ===
if __name__ == "__main__":
    logger.info("Starting Flask application initialization...")
    try:
        load_model_and_data()
        logger.info("Model and data loaded successfully. Starting Flask server...")
        is_debug = os.environ.get("FLASK_DEBUG", "False").lower() == "true"
        # Add a small delay in debug mode if needed to simulate load?
        # if is_debug: app.before_request(lambda: time.sleep(0.1))
        app.run(host='0.0.0.0', port=5000, debug=is_debug, threaded=not is_debug)
    except SystemExit as se: logger.critical(f"Initialization failed: {se}. Flask app will not start.")
    except Exception as e: logger.critical(f"Unexpected error during initialization: {e}", exc_info=True); print("\n !!! FLASK APP FAILED TO INITIALIZE !!! \n")